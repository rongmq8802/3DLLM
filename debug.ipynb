{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1266e8040f194157b7e639da73143984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "from lavis.common.registry import registry\n",
    "from lavis.models.blip2_models.blip2 import Blip2Base, disabled_train\n",
    "from lavis.models.blip2_models.modeling_opt import OPTForCausalLM, OPTConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import lavis.models.blip2_models.llama as llama\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "# llama_model_path = \"/home/tdt/Llama/\"\n",
    "llama_model_path = \"/home/tdt/llama-7b/\"\n",
    "# llama_model_path = \"/data4/tdt/3DLLM/llama-7b/\"\n",
    "config = llama.LLaMAConfig.from_pretrained(llama_model_path)\n",
    "llama_tokenizer = llama.LLaMATokenizer.from_pretrained(llama_model_path)\n",
    "llama_model = llama.LLaMAForCausalLM.from_pretrained(llama_model_path, torch_dtype=torch.float16, config=config, state_dict=None)\n",
    "# llama_model.save_pretrained(save_directory=\"/home/tdt/llama-7b/\", max_shard_size=\"1GB\")\n",
    "# llama_tokenizer.save_pretrained(save_directory=\"/home/tdt/llama-7b/\")\n",
    "# llama_model.to(device)\n",
    "# llama_model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec534afda7ff4697bf6577bb872c1654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ChatGLMForConditionalGeneration(\n",
       "  (transformer): ChatGLMModel(\n",
       "    (word_embeddings): Embedding(130528, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (3): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (4): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (5): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (6): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (7): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (8): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (9): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (10): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (11): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (12): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (13): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (14): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (15): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (16): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (17): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (18): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (19): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (20): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (21): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (22): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (23): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (24): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (25): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (26): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (27): GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=130528, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from lavis.common.registry import registry\n",
    "from lavis.models.blip2_models.blip2 import Blip2Base, disabled_train\n",
    "from lavis.models.blip2_models.modeling_opt import OPTForCausalLM, OPTConfig\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "chatglm_model_path = \"/home/tdt/chatglm-6b/\"\n",
    "chatglm_tokenizer = AutoTokenizer.from_pretrained(chatglm_model_path, use_fast=False, trust_remote_code=True)\n",
    "chatglm_model = AutoModel.from_pretrained(chatglm_model_path, torch_dtype=torch.float16, trust_remote_code=True).half()\n",
    "chatglm_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65831, 75137, 63902, 64130, 67298, 65794, 69839, 63823, 130001, 130004]\n",
      "[71492, 67194, 64258, 63907, 75137, 63902, 64130, 63823, 130001, 130004]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"请描述一下这个三维点云。\"\n",
    "answer = \"这个三维点云中有一张桌。\"\n",
    "a_ids = chatglm_tokenizer.encode(answer, add_special_tokens=True)\n",
    "b_ids = chatglm_tokenizer.encode(prompt, add_special_tokens=True)\n",
    "print(a_ids)\n",
    "print(b_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_ids:  [65831, 75137, 63902, 64130, 67298, 65794, 69839, 63823]\n",
      "b_ids:  [71492, 67194, 64258, 63907, 75137, 63902, 64130, 63823]\n",
      "input_ids:  [71492, 67194, 64258, 63907, 75137, 63902, 64130, 63823, 130001, 130004, 65831, 75137, 63902, 64130, 67298, 65794, 69839, 63823, 130005]\n",
      "len(input_ids):  19\n",
      "9\n",
      "------------------\n",
      "input_ids:  [71492, 67194, 64258, 63907, 75137, 63902, 64130, 63823, 130001, 130004, 65831, 75137, 63902, 64130, 67298, 65794, 69839, 63823, 130005, 3, 3, 3, 3, 3, 3]\n",
      "len(input_ids):  25\n",
      "labels:  [-100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 65831, 75137, 63902, 64130, 67298, 65794, 69839, 63823, 130005, -100, -100, -100, -100, -100, -100]\n",
      "len(labels):  25\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 25\n",
    "prompt = \"请描述一下这个三维点云。\"\n",
    "answer = \"这个三维点云中有一张桌。\"\n",
    "a_ids = chatglm_tokenizer.encode(answer, add_special_tokens=False)\n",
    "b_ids = chatglm_tokenizer.encode(prompt, add_special_tokens=False)\n",
    "input_ids = chatglm_tokenizer.build_inputs_with_special_tokens(b_ids, a_ids)\n",
    "context_length = input_ids.index(chatglm_tokenizer.bos_token_id)\n",
    "print(\"a_ids: \", a_ids)\n",
    "print(\"b_ids: \", b_ids)\n",
    "print(\"input_ids: \", input_ids)\n",
    "print(\"len(input_ids): \", len(input_ids))\n",
    "print(context_length)\n",
    "mask_position = context_length - 1\n",
    "labels = [-100] * context_length + input_ids[mask_position + 1:]\n",
    "# print(\"labels: \", labels)\n",
    "# print(\"len(labels): \", len(labels))\n",
    "\n",
    "pad_len = max_seq_length - len(input_ids)\n",
    "input_ids = input_ids + [chatglm_tokenizer.pad_token_id] * pad_len\n",
    "labels = labels + [chatglm_tokenizer.pad_token_id] * pad_len\n",
    "labels = [(-100 if l == chatglm_tokenizer.pad_token_id else l) for l in labels]\n",
    "print(\"------------------\")\n",
    "print(\"input_ids: \", input_ids)\n",
    "print(\"len(input_ids): \", len(input_ids))\n",
    "print(\"labels: \", labels)\n",
    "print(\"len(labels): \", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:  [71492, 67194, 64258, 63907, 75137, 63902, 64130, 63823, 130001, 130004, 65831, 75137, 63902, 64130, 67298, 65794, 69839, 63823, 130005]\n",
      "context_length:  9\n"
     ]
    }
   ],
   "source": [
    "prompt_token = chatglm_tokenizer.tokenize(prompt)\n",
    "tgt_token = chatglm_tokenizer.tokenize(answer)\n",
    "tokens = prompt_token + [\"[gMASK]\", \"<sop>\"] + tgt_token + [\"<eop>\"]\n",
    "input_ids = chatglm_tokenizer.convert_tokens_to_ids(tokens)\n",
    "context_length = input_ids.index(chatglm_tokenizer.bos_token_id)\n",
    "print(\"input_ids: \", input_ids)\n",
    "print(\"context_length: \", context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt_token:  {'input_ids': [[65831, 75137, 63902, 64130, 67298, 65794, 74212, 63823], [65831, 75137, 63902, 64130, 67298, 65794, 74212, 63936, 66979, 77788, 63823]]}\n"
     ]
    }
   ],
   "source": [
    "answer = [\"这个三维点云中有一张桌子。\", \"这个三维点云中有一张桌子以及一把椅子。\"]\n",
    "tgt_token = chatglm_tokenizer(answer, add_special_tokens=False)\n",
    "print(\"tgt_token: \", tgt_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:  tensor([[65831, 75137, 63902, 64130, 67298, 65794, 74212, 63823, 20003, 20003,\n",
      "         20003],\n",
      "        [65831, 75137, 63902, 64130, 67298, 65794, 74212, 63936, 66979, 77788,\n",
      "         63823]])\n",
      "input_ids.shape:  torch.Size([2, 11])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "answer = [\"这个三维点云中有一张桌子。\", \"这个三维点云中有一张桌子以及一把椅子。\"]\n",
    "tgt_token = chatglm_tokenizer(answer, add_special_tokens=False)\n",
    "input_ids = pad_sequence([torch.tensor(chatglm_tokenizer.encode(t, add_special_tokens=False)) for t in answer], batch_first=True, padding_value=20003) \n",
    "print(\"input_ids: \", input_ids)\n",
    "print(\"input_ids.shape: \", input_ids.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:  [71492, 67194, 64258, 63907, 75137, 63902, 64130, 63823, 130001, 130004, 65831, 75137, 63902, 64130, 67298, 65794, 74212, 63823, 130005, 3, 3, 3, 3, 3, 3]\n",
      "labels:  [-100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 65831, 75137, 63902, 64130, 67298, 65794, 74212, 63823, 130005, -100, -100, -100, -100, -100, -100]\n",
      "len(input_ids):  25\n",
      "len(labels):  25\n",
      "------------------\n",
      "input_ids:  [71492, 67194, 64258, 63907, 75137, 63902, 64130, 63823, 130001, 130004, 65831, 75137, 63902, 64130, 67298, 65794, 74212, 63936, 66979, 77788, 63823, 130005, 3, 3, 3]\n",
      "labels:  [-100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 65831, 75137, 63902, 64130, 67298, 65794, 74212, 63936, 66979, 77788, 63823, 130005, -100, -100, -100]\n",
      "len(input_ids):  25\n",
      "len(labels):  25\n",
      "------------------\n",
      "input_ids:  tensor([[ 71492,  67194,  64258,  63907,  75137,  63902,  64130,  63823, 130001,\n",
      "         130004,  65831,  75137,  63902,  64130,  67298,  65794,  74212,  63823,\n",
      "         130005,      3,      3,      3,      3,      3,      3],\n",
      "        [ 71492,  67194,  64258,  63907,  75137,  63902,  64130,  63823, 130001,\n",
      "         130004,  65831,  75137,  63902,  64130,  67298,  65794,  74212,  63936,\n",
      "          66979,  77788,  63823, 130005,      3,      3,      3]])\n",
      "labels:  tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "         130004,  65831,  75137,  63902,  64130,  67298,  65794,  74212,  63823,\n",
      "         130005,   -100,   -100,   -100,   -100,   -100,   -100],\n",
      "        [  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "         130004,  65831,  75137,  63902,  64130,  67298,  65794,  74212,  63936,\n",
      "          66979,  77788,  63823, 130005,   -100,   -100,   -100]])\n",
      "input_ids.shape:  torch.Size([2, 25])\n",
      "labels.shape:  torch.Size([2, 25])\n"
     ]
    }
   ],
   "source": [
    "text = [\"这个三维点云中有一张桌子。\", \"这个三维点云中有一张桌子以及一把椅子。\"]\n",
    "prompt = [\"请描述一下这个三维点云。\"] * len(text)\n",
    "input_ids_list, labels_list = [], []\n",
    "for t, p in zip(text, prompt):\n",
    "    a_ids = chatglm_tokenizer.encode(t, add_special_tokens=False)\n",
    "    b_ids = chatglm_tokenizer.encode(p, add_special_tokens=False)\n",
    "    input_ids = chatglm_tokenizer.build_inputs_with_special_tokens(b_ids, a_ids)\n",
    "    context_length = input_ids.index(chatglm_tokenizer.bos_token_id)\n",
    "    mask_position = context_length - 1\n",
    "    labels = [-100] * context_length + input_ids[mask_position + 1:]\n",
    "    pad_len = max_seq_length - len(input_ids)\n",
    "    input_ids = input_ids + [chatglm_tokenizer.pad_token_id] * pad_len\n",
    "    labels = labels + [-100] * pad_len\n",
    "    labels = [(-100 if l == chatglm_tokenizer.pad_token_id else l) for l in labels]\n",
    "    input_ids_list.append(input_ids)\n",
    "    labels_list.append(labels)\n",
    "    print(\"input_ids: \", input_ids)\n",
    "    print(\"labels: \", labels)\n",
    "    print(\"len(input_ids): \", len(input_ids))\n",
    "    print(\"len(labels): \", len(labels))\n",
    "    print(\"------------------\")\n",
    "\n",
    "input_ids = torch.tensor(input_ids_list)\n",
    "labels = torch.tensor(labels_list)\n",
    "print(\"input_ids: \", input_ids)\n",
    "print(\"labels: \", labels)\n",
    "print(\"input_ids.shape: \", input_ids.shape)\n",
    "print(\"labels.shape: \", labels.shape)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pointcept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
