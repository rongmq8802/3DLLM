model:
  arch: blip2_llama
  load_finetuned: False
  load_pretrained: True      # 目前为了调试设为了False 实际使用时要设为True 而且要为下面的 pretrained 参数赋值
  pretrained: "/public/public_data/3DLLM/pretrained_model/stage1/cap_and_structure.pth"
  finetuned: "/public/public_data/3DLLM/pretrained_model/stage1/cap_and_structure.pth"
  
  # point transformer encoder
  point_cloud_encoder_model: "point_transformer"
  point_cloud_encoder_model_path: "/home/rmq2/3DLLM_2/Combine_final/model/resave_best.pth"
  drop_path_rate: 0
  use_grad_checkpoint: False
  freeze_cloud_encoder: True


  # Q-Former
  num_query_token: 32
  qformer_encoder_layer: 12

  # Llama
  llama_model: "llama_chinese"        # 名字, 目前没有作用
  # pretrained_llama_path: "/public/public_data/3DLLM/pretrained_model/llama-chinese-13b/"           # 预训练的llama模型的保存路径, 注意是一个文件夹, 绝对路径
  # pretrained_llama_path: "/public/public_data/3DLLM/pretrained_model/llama-7b/"
  pretrained_llama_path: "/public/public_data/3DLLM/pretrained_model/wair_13B_v6p8_20230602/checkpoint-6000/"
  max_txt_len: 100                    # 这是分词器 tokenizer 输出的token的最大长度

# 这里也有vis_processor 和 text_processor, 但是不要和数据集配置里的那两个搞混了
# 这两个的作用是如果想要单独使用模型，也就是输入一个点云+文字，输出一段文字的时候，就需要这个了
# 一般用在 jupyter notebook 里，就是实时的输入输出，不需要额外使用dataloader
preprocess:                     
  vis_processor:
    train:
      name: "cloud_train"
      max_size: 100000
    eval:                       # eval 等价于 val + test
      name: "cloud_test"
      max_size: 100000
  text_processor:
    train:
      name: "chinese_caption"   
      max_words: 100
    eval:
      name: "chinese_caption"
      max_words: 100