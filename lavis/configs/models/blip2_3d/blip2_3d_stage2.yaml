model:
  arch: blip2_llama
  load_finetuned: False
  load_pretrained: False      # 目前为了调试设为了False 实际使用时要设为True 而且要为下面的 pretrained 参数赋值
  pretrained: "/home/tdt/3DLLM/lavis/output/BLIP2/Pretrain_stage1/20230417095/checkpoint_99.pth"             
  finetuned: ""
  
  # point transformer encoder
  point_cloud_encoder_model: "point_transformer"    # point_transformer
  point_cloud_encoder_model_path: "/home/tdt/PointTransformer/scannet-512-model_best.pth"
  drop_path_rate: 0
  use_grad_checkpoint: False
  freeze_cloud_encoder: True


  # Q-Former
  num_query_token: 32
  qformer_encoder_layer: 2

  # Llama
  llama_model: "llama_chinese"        # 名字, 目前没有作用
  pretrained_llama_path: "/home/tdt/llama-7b/"           # 预训练的llama模型的保存路径, 注意是一个文件夹, 绝对路径
  max_txt_len: 100                    # 这是分词器 tokenizer 输出的token的最大长度

# 这里也有vis_processor 和 text_processor, 但是不要和数据集配置里的那两个搞混了
# 这两个的作用是如果想要单独使用模型，也就是输入一个点云+文字，输出一段文字的时候，就需要这个了
# 一般用在 jupyter notebook 里，就是实时的输入输出，不需要额外使用dataloader
preprocess:                     
  vis_processor:
    train:
      name: "cloud_train"
      max_size: 80000
    eval:                       # eval 等价于 val + test
      name: "cloud_test"
      max_size: 80000
  text_processor:
    train:
      name: "chinese_caption"   
      max_words: 100
    eval:
      name: "chinese_caption"
      max_words: 100