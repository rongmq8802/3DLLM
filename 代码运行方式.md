# 代码运行

代码整体的运行方式和原本的BLIP2是相同的，配置文件等也是类似的，所以在不同的电脑上运行需要对配置文件进行修改，尤其是配置文件中一些模型的绝对路径。配置文件的修改具体参见下一节 **代码细节** 部分。

- 第一阶段训练运行的命令为

```bash
cd 3DLLM	# 进入代码所在目录
python -m torch.distributed.run train_3d.py --cfg-path ./3DLLM/lavis/projects/blip2_3d/train/pretrain_stage1.yaml
```

- 第二阶段训练运行的命令为

```bash
cd 3DLLM	# 进入代码所在目录
python -m torch.distributed.run train_3d.py --cfg-path ./3DLLM/lavis/projects/blip2_3d/train/pretrain_stage2.yaml
```

- 在服务器上搭建一个后台服务，可以通过 http 的POST进行访问的代码是

```bash
cd 3DLLM
python demo.py
```

​	启动服务后，在浏览器访问 `xxx.xxx.xxx.xxx:5000/caption` 即可。其中POST的内容为以下json格式

```json
{
"cloud_path":"/public/public_data/3DLLM/merge/train/Area_1/office_9.pth",	// 点云的路径
"prompt":"请描述一下这个三维场景。",	// 这个场景对应的prompt
"max_sentences":3,		// 可选参数，最多输出N句话，每句话是以句号作为判定。
"max_length":100		// 可选参数，最多输出N个字，符号也算
}
```

- 直接使用预训练好的模型对点云进行描述的代码在 `3d_caption.py` 中，但其中比较杂乱，以下梳理了主要逻辑

```python
import torch
import numpy as np
import plyfile
from typing import Dict, List
from lavis.models import load_model_and_preprocess

def load_point_cloud(path:str) -> Dict[str, torch.Tensor]:
    """
    从文件中读取点云
    path: 点云路径,绝对路径
    return: 点云, 字典类型, 包含 "coord", "color", "semantic_gt" 三个key
    """
    file_type = path.split(".")[-1]
    if file_type == "pth":
        cloud = torch.load(path)
        # 专门针对strucutred3D数据集的处理, 因为保存的 pth 是个tuple的格式, 而且点云的尺度很大, 需要归一化
        if(isinstance(cloud, tuple)):
            cloud = {"coord": cloud[0], "color": cloud[1], "semantic_gt": cloud[2]}
            cloud["color"] = ((cloud["color"] + 1) * 127.5).astype(np.uint8)
            cloud["color"] = cloud["color"].astype(np.float64)
            cloud["coord"] = cloud["coord"].astype(np.float64)
            # 把 coord 中的值归一化到 [-5, 5] 之间
            max_value = np.max(cloud["coord"])
            min_value = np.min(cloud["coord"])
            final_value = max(abs(max_value), abs(min_value))
            cloud["coord"] = cloud["coord"] / final_value  * 5.0

        # "coord" "color" "semantic_gt"
        if "semantic_gt" in cloud.keys():
            cloud["semantic_gt"] = cloud["semantic_gt"].reshape([-1])
            cloud["semantic_gt"] = cloud["semantic_gt"].astype(np.int64)
    elif file_type == "ply":
        cloud = {}
        plydata = plyfile.PlyData().read(path)
        points = np.array([list(x) for x in plydata.elements[0]])
        coords = np.ascontiguousarray(points[:, :3]).astype(np.float64)
        colors = np.ascontiguousarray(points[:, 3:6]).astype(np.float64)
        semantic_gt = np.zeros((coords.shape[0]), dtype=np.int64)
        max_value = np.max(coords)
        min_value = np.min(coords)
        final_value = max(abs(max_value), abs(min_value))
        # S3DIS/ScanNet/Structure3D三个数据集的尺度没有进行统一。
        # 尤其对于Structure3D数据集，尺度非常大，所以我们在这设置一个阈值，
        # 若final_value大于1000，则认为是Structure3D数据集，然后将点云的坐标归一化到[-5, 5]的范围内
        if final_value > 1000:
            cloud["coord"] = coords / final_value * 5.0
        else:
            cloud["coord"] = coords
        cloud["color"] = colors
        cloud["semantic_gt"] = semantic_gt
    else:
        raise ValueError("file type {} not supported".format(file_type))
    
    return cloud

device = torch.device("cuda")
model, vis_processors, _ = load_model_and_preprocess(
    name="blip2_llama", model_type="blip2_3d_caption", is_eval=True, device=device
)
cloud_path = "/public/public_data/3DLLM/merge/train/scene0324_00.pth"
cloud = load_point_cloud(cloud_path)
cloud = vis_processors["eval"](cloud)

for k in cloud.keys():
    if(isinstance(cloud[k], torch.Tensor)):
        cloud[k] = cloud[k].to(device)
        cloud[k] = cloud[k].unsqueeze(0)

result = model.generate_with_hidden_prompt({"cloud":cloud, "text_input": prompt}, max_length=100, num_beams=1)
result_processed = post_process(result, max_sentences=2)
print(result_processed)
```

- **使用 ChatGLM模型**

  使用 ChatGLM模型进行点云描述任务和使用 LLaMA 是一样的，只需要在以上代码中修改一行即可。在 `3d_caption_chatglm.py` 中有更详细的代码

  ```python
  model, vis_processors, _ = load_model_and_preprocess(
      name="blip2_chatglm", model_type="blip2_3d_caption_chatglm", is_eval=True, device=device
  )
  ```

  

# 代码细节

## 1. 配置文件

遵循BLIP2默认的配置文件组织方式，我们的配置文件都放在了以下几个路径下

- `3DLLM/lavis/projects/blip2_3d/train/pretrain_stage1.yaml` 第一阶段训练时使用的整体的配置文件

- `3DLLM/lavis/configs/models/blip2_3d/blip2_3d_stage1.yaml` 第一阶段训练时Q-former的配置，作为上一个配置文件的补充，包含了Q-former详细的结构配置，以及我们的三维backbone的预训练模型的位置。**不同电脑请相应的修改路径**

  

- `3DLLM/lavis/projects/blip2_3d/train/pretrain_stage2.yaml` 第二阶段训练时使用的整体的配置文件

- `3DLLM/lavis/configs/models/blip2_3d/blip2_3d_stage2.yaml` 第二阶段训练时Q-former的配置，作为上一个配置文件的补充，包含了Q-former详细的结构配置、三维backbone的预训练模型位置、第一阶段Q-former训练结果的位置、LLaMA模型位置。**不同电脑请相应的修改路径**

- `3DLLM/lavis/configs/models/blip2_3d/blip2_3d_stage2_chatglm.yaml`  使用ChatGLM进行二阶段训练时的配置，整体配置和 LLaMA 相同，只是模型位置变化了。**不同电脑请相应的修改路径**



- `3DLLM/lavis/configs/models/blip2_3d/blip2_3d_caption.yaml` 使用我们的模型进行点云描述时的配置文件，也就是以下这行代码运行时读取的Q-former参数。这些参数整体上和二阶段训练时的类似。 **不同电脑请相应的修改路径**

  ```python
  load_model_and_preprocess(name="blip2_llama", model_type="blip2_3d_caption", is_eval=True, device=device)
  ```

- `3DLLM/lavis/configs/models/blip2_3d/blip2_3d_caption_chatglm.yaml` 使用基于ChatGLM训练的模型进行点云描述时的配置文件，也就是以下代码运行时读取的Q-former参数

  ```python
  load_model_and_preprocess(name="blip2_chatglm", model_type="blip2_3d_caption_chatglm", is_eval=True, device=device)
  ```

  



- `3DLLM/lavis/configs/datasets/point_cloud/defaults_cap.yaml` 是数据集的配置文件，第一阶段和第二阶段都是使用同样的数据集。如果希望两个阶段使用不同的数据集，那么就要手动修改配置文件后，然后再分别运行一二阶段。这里包含了 `train \ test \ val` 三类，但是在我们的代码中，`test` 是完全没用的，第一阶段用了 `train + val` 第二阶段只用了 `train` 。**不同电脑请相应的修改路径**



## 2. 代码

我们的代码文件同样遵循BLIP2默认的组织方式，主要在以下几个路径中 

- `lavis/models/blip2_models` 是我们的主要代码，Q-former以及三维backbone相关的代码都在其中。注意，这个名字和原本的BLIP2文件夹名字是相同的，我们把原本的BLIP2放到了 `lavis/models/blip2_models_reserve` 中
- `3DLLM/lavis/datasets/builders/cloud_text_pair_builder.py` 是建立 Dataset 相关的代码，这里指定了默认的数据集配置文件路径是 `configs/datasets/point_cloud/defaults_cap.yaml` 
- `3DLLM/lavis/datasets/datasets/cloud_text_pair_datasets.py` 是数据集本身的代码，这里包含了读取数据集的 json，读取点云，是真正的 dataloader
- `3DLLM/lavis/processors/cloud_processors.py` 是对点云和文字进行预处理的相关代码，包含了对点云进行数据增强以及随机采样等，对文字主要是根据长度限制进行截断
- `3DLLM/lavis/tasks/pointcloud_text_pretrain.py` 是两个阶段的task相关的代码，因为要实现自定义的验证方式，所以对 `BaseTask` 的一部分函数进行了重载

为了更方便的调试，以及设置一些自定义的操作，我们对 lavis 本身的基础代码也进行了一些修改，但这些修改都是不太重要的，理论上使用原本的 lavis 代码也应该能成功运行起来。以下是对 lavis 进行的修改

1. 对 `runner_base.py` 的修改
   - 无论有没有验证集，训练的模型都每隔一轮保存一次
   - 让模型能在CPU上运行
   - log中增加整体运行时间，把log保存为json时可以存中文了

2. 对 `base_task.py` 的修改
   - 在二阶段训练的时候，为了能直观的看到LLM生成的语言效果，我们把每个样本在不同的epoch由LLM生成的语句都保存下来，并且按照epoch的顺序输出到txt中

## 3. 数据

我们的数据完全存储在 json 中，因此每个数据集都只有一个json即可。每个json的内容如下

```json
{
	"path/to/pointcloud1.pth": ["This is caption 1", "This is prompt 1"], 
    "path/to/pointcloud2.pth": ["This is caption 2", "This is prompt 2"],
    "path/to/pointcloud3.pth": ["This is caption 3"],
    "path/to/pointcloud4.pth": "This is caption 4"
}
```

json 中的每个键值对中，键(key) 是点云的路径，之后就会根据这个路径读取点云；值(value) 是这个点云对应的描述(caption) 以及训练时需要使用的提示(prompt)，提示是可以不写的，如果不写那么就用默认的提示 *"请描述一下这个三维场景。"* 默认提示可以在模型的配置文件中修改，也就是`3DLLM/lavis/projects/blip2_3d/train/pretrain_stage2.yaml` 中的 `model.prompt` 参数。


hhhhh










